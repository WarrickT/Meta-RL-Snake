{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07759295",
   "metadata": {},
   "source": [
    "# How is the RL structured? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ad7e5",
   "metadata": {},
   "source": [
    "## Attempt #1: Naive! This one sucked. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6039a7",
   "metadata": {},
   "source": [
    "### The first state-reward structure was the following, trained on 5000 episodes:\n",
    "\n",
    "#### State Variables:\n",
    "- danger_front: If going forward results in dying\n",
    "- danger_left: If going left results in dying\n",
    "- danger_right: If going right results in dying\n",
    "- direction_index: Direction of going UP/DOWN/LEFT/RIGHT, with respect to the entire map\n",
    "- apple_up_down: Relative direction of apple\n",
    "- apple_left_right: Relative direction of apple\n",
    "\n",
    "#### Reward System: \n",
    "- +1 points for apple\n",
    "- -1 points for dying \n",
    "- -0.1 for achieving neither (Stalling)\n",
    "\n",
    "#### Findings\n",
    "- For a grid size of 15x15, the highest score I've obtained was around ~38 apples, which is not great in terms of achieving my goal of an 80% map coverage. \n",
    "- The state is likely too specific! The model probably isn't doing any actual, but just guessing as most scenarios will seem \"foreign\" to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbd636b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec45f358",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
